{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#Importing the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skorch import NeuralNetClassifier\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "#print count of column Class\n",
    "print(df['Class'].value_counts())\n",
    "\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "#Splitting the data into training and testing data\n",
    "# Utiliser train_Test_split de scikit-learn\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "batch_size = 2**11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création d'un dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y      \n",
    "    \n",
    "#Converting the data into tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "X_train = CustomDataSet(X_train, y_train)\n",
    "X_test = CustomDataSet(X_test, y_test) \n",
    "\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création d'un data équilibré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_train)\n\u001b[1;32m      3\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m class_counts\n\u001b[0;32m----> 4\u001b[0m sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43mclass_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m sampler \u001b[38;5;241m=\u001b[39m WeightedRandomSampler(weights\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(sample_weights, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), \n\u001b[1;32m      6\u001b[0m                                 num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(sample_weights), \n\u001b[1;32m      7\u001b[0m                                 replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Création des datasets d'entraînement et de test avec conversion explicite en float32\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Calcul des poids de classe pour gérer un dataset déséquilibré\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = class_weights[y_train]\n",
    "sampler = WeightedRandomSampler(weights=torch.tensor(sample_weights, dtype=torch.float32), \n",
    "                                num_samples=len(sample_weights), \n",
    "                                replacement=True)\n",
    "\n",
    "# Création des datasets d'entraînement et de test avec conversion explicite en float32\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                              torch.tensor(y_train, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), \n",
    "                             torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "# Création des DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1000, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_function(classes, confusion_matrix):\n",
    "    print(\"Confusion Matrix\")\n",
    "    # Créer£ les en-têtes de la table avec les classes prédites\n",
    "    headers = [\"\"] + [f\"Predicted: {cls}\" for cls in classes.index]\n",
    "\n",
    "    # Créer les lignes de la table avec les valeurs de la matrice de confusion\n",
    "    rows = [\n",
    "        [f\"Class: {classes.index[i]} - {classes[i]}\"] + list(confusion_matrix[i])\n",
    "        for i in range(len(classes))\n",
    "    ]\n",
    "\n",
    "    # Afficher la table\n",
    "    print(tabulate(rows, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation du model\n",
    "class MyFlexibleModel(nn.Module):\n",
    "    def __init__(self, in_features, hidden_layers, num_unit, out_features, dropout_rate=0.5):\n",
    "        super(MyFlexibleModel, self).__init__()\n",
    "        \n",
    "        # Create a list to store the layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(in_features, num_unit))\n",
    "        \n",
    "        # Add the hidden layers\n",
    "        for i in range(1, hidden_layers):\n",
    "            self.layers.append(nn.Linear(num_unit, num_unit))\n",
    "            self.layers.append(nn.BatchNorm1d(num_unit))\n",
    "            self.layers.append(nn.ReLU())  \n",
    "        \n",
    "        # Ajouter la couche de sortie\n",
    "        self.layers.append(nn.Linear(num_unit, out_features))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.layers[i](x)\n",
    "            # x = self.dropout(x)\n",
    "        \n",
    "        x = self.layers[-1](x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "\n",
    "    # avoir les attributs du model\n",
    "    print(model)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        losses_train = 0\n",
    "        correct_predictions, total_predictions = 0, 0 \n",
    "        \n",
    "        y_pred_train = []\n",
    "        y_true_train = []\n",
    "        \n",
    "    \n",
    "        for inputs, labels in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()  # Reset gradients to zero\n",
    "            outputs_train = model(inputs)  # Pass data through the model\n",
    "            loss = criterion(outputs_train.squeeze(), labels)  # Calculate loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # Cumulate loss from each bach\n",
    "            losses_train += loss.item()\n",
    "            \n",
    "            # Calculate the accuracy\n",
    "            #Apply sigmoid to the outputs cause outputs are logits\n",
    "            predictions = (outputs_train.squeeze()  >= 0.5).float()  # convert probabilities to binary predictions\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0) \n",
    "            \n",
    "            # Append the predictions to the list\n",
    "            y_pred_train.extend(predictions.cpu().numpy())\n",
    "            y_true_train.extend(labels.cpu().numpy())\n",
    "        \n",
    "        losses = losses_train / len(train_loader)\n",
    "        train_loss.append(losses)\n",
    "        \n",
    "        train_acc = correct_predictions / total_predictions\n",
    "        #Evaluation Mode\n",
    "        model.eval() \n",
    "        \n",
    "        y_pred_test, y_true_test = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #loss and accuracy for the validation set\n",
    "            losses_test = 0\n",
    "            test_acc = 0\n",
    "            \n",
    "            correct_predictions_label = 0\n",
    "            total_predictions = 0\n",
    "            \n",
    "            for inputs_test, labels_test in test_loader:\n",
    "                outputs_labels = model(inputs_test)\n",
    "                loss = criterion(outputs_labels.squeeze(), labels_test)\n",
    "                losses_test += loss.item()\n",
    "                \n",
    "                prediction_test = (outputs_labels.squeeze() >= 0.5).float()\n",
    "\n",
    "                correct_predictions_label += (prediction_test == labels_test).sum().item()\n",
    "                total_predictions += labels_test.size(0)\n",
    "                \n",
    "                y_pred_test.extend(prediction_test.numpy())\n",
    "                y_true_test.extend(labels_test.numpy())\n",
    "            test_acc = correct_predictions_label / total_predictions\n",
    "            \n",
    "            losses_test = losses_test / len(test_loader)\n",
    "            test_loss.append(losses_test)\n",
    "            \n",
    "            acc_train.append(train_acc)\n",
    "            acc_test.append(test_acc)\n",
    "\n",
    "        print('='*100)   \n",
    "        print(f'Epoch: {epoch+1} - Loss Train : {losses:4f} - Loss Val : {losses_test:.4f} - Acc Train : {train_acc:.4f} - Acc Val : {test_acc:.4f}')\n",
    "            \n",
    "    end_time = time.time() \n",
    "\n",
    "    print(f\"Training time: {end_time - start_time}s\")\n",
    "    # Graphe pour les pertes (Loss)\n",
    "    return train_loss, test_loss, acc_train, acc_test\n",
    "\n",
    "\n",
    "def show_plt(train_loss, test_loss, acc_train, acc_test):\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.plot(train_loss, label='Train Loss', color='blue')\n",
    "    ax1.plot(test_loss, label='Test Loss', color='red')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    \n",
    "    fig2, ax2 = plt.subplots()\n",
    "    ax2.plot(acc_train, label='Train Accuracy', color='blue')\n",
    "    ax2.plot(acc_test, label='Test Accuracy', color='red')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyFlexibleModel(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (5): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (8): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (11): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (14): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (17): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "    (19): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (20): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (21): ReLU()\n",
      "    (22): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (23): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU()\n",
      "    (25): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (26): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU()\n",
      "    (28): Linear(in_features=20, out_features=1, bias=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "====================================================================================================\n",
      "Epoch: 1 - Loss Train : 0.700840 - Loss Val : 1.0540 - Acc Train : 0.5882 - Acc Val : 0.0016\n",
      "====================================================================================================\n",
      "Epoch: 2 - Loss Train : 0.607740 - Loss Val : 0.8436 - Acc Train : 0.8321 - Acc Val : 0.1507\n",
      "====================================================================================================\n",
      "Epoch: 3 - Loss Train : 0.544845 - Loss Val : 0.6671 - Acc Train : 0.9598 - Acc Val : 0.5792\n",
      "====================================================================================================\n",
      "Epoch: 4 - Loss Train : 0.495657 - Loss Val : 0.6088 - Acc Train : 0.9857 - Acc Val : 0.9958\n",
      "====================================================================================================\n",
      "Epoch: 5 - Loss Train : 0.454559 - Loss Val : 0.4648 - Acc Train : 0.9961 - Acc Val : 0.9987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x11143f2b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/cardfraud/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Epoch: 6 - Loss Train : 0.418345 - Loss Val : 0.3498 - Acc Train : 0.9980 - Acc Val : 0.9986\n",
      "====================================================================================================\n",
      "Epoch: 7 - Loss Train : 0.383336 - Loss Val : 0.3170 - Acc Train : 0.9982 - Acc Val : 0.9987\n",
      "====================================================================================================\n",
      "Epoch: 8 - Loss Train : 0.351501 - Loss Val : 0.2509 - Acc Train : 0.9982 - Acc Val : 0.9987\n",
      "====================================================================================================\n",
      "Epoch: 9 - Loss Train : 0.322103 - Loss Val : 0.2244 - Acc Train : 0.9982 - Acc Val : 0.9987\n",
      "====================================================================================================\n",
      "Epoch: 10 - Loss Train : 0.294737 - Loss Val : 0.2436 - Acc Train : 0.9982 - Acc Val : 0.9987\n",
      "====================================================================================================\n",
      "Epoch: 11 - Loss Train : 0.268628 - Loss Val : 0.1450 - Acc Train : 0.9982 - Acc Val : 0.9987\n",
      "====================================================================================================\n",
      "Epoch: 12 - Loss Train : 0.244564 - Loss Val : 0.1749 - Acc Train : 0.9982 - Acc Val : 0.9987\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m     31\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m train_loss, test_loss, acc_train, acc_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m show_plt(train_loss, test_loss, acc_train, acc_test)\n",
      "Cell \u001b[0;32mIn[11], line 59\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     56\u001b[0m correct_predictions_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     57\u001b[0m total_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs_test, labels_test \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m     60\u001b[0m     outputs_labels \u001b[38;5;241m=\u001b[39m model(inputs_test)\n\u001b[1;32m     61\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs_labels\u001b[38;5;241m.\u001b[39msqueeze(), labels_test)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cardfraud/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cardfraud/lib/python3.9/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = [\n",
    "    {\n",
    "        \"hidden_layers\": 10,\n",
    "        \"num_unit\": 20,\n",
    "        \"lr\": 0.0001\n",
    "    },\n",
    "    {\n",
    "        \"hidden_layers\": 10,\n",
    "        \"num_unit\": 40,\n",
    "        \"lr\": 0.0001\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        \"hidden_layers\": 10,\n",
    "        \"num_unit\": 60,\n",
    "        \"lr\": 0.0001\n",
    "    },\n",
    "    {\n",
    "        \"hidden_layers\": 10,\n",
    "        \"num_unit\": 80,\n",
    "        \"lr\": 0.0001\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        \"hidden_layers\": 10,\n",
    "        \"num_unit\": 100,\n",
    "        \"lr\": 0.0001\n",
    "        \n",
    "    },\n",
    "]\n",
    "\n",
    "for param in params: \n",
    "    model = MyFlexibleModel(X.shape[1], param[\"hidden_layers\"], param[\"num_unit\"], 1)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    train_loss, test_loss, acc_train, acc_test = train_model(model, train_loader, criterion, optimizer, 30)\n",
    "    show_plt(train_loss, test_loss, acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cardfraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
